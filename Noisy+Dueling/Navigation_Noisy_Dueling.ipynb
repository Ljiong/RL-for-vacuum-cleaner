{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "from dqn_agent_Noisy_Dueling import Agent\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_PATH = \"environment-MAC/en.app\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialise customised Banana Collecter environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_env(ENV_PATH):\n",
    "    # env = UnityEnvironment(file_name=ENV_PATH)\n",
    "    env = UE(base_port=5004,file_name=ENV_PATH, seed=1, side_channels=[])\n",
    "    env.step()\n",
    "    # in this project, we are only using one agent, so we will only work on the first `brain` in the environmet\n",
    "    # get the default brain\n",
    "    # brain_name = env.brain_names[0]\n",
    "    brain_name = list(env.behavior_specs.keys())[0]\n",
    "    # brain = env.brains[brain_name]\n",
    "    brain = env.behavior_specs[brain_name]\n",
    "    return env, brain, brain_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0618 00:55:56.831087000 8607102464 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n"
     ]
    }
   ],
   "source": [
    "env, brain, brain_name = initialise_env(ENV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = env.behavior_specs['My Behavior?team=0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = list(brain.action_spec)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = len(brain.observation_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObservationSpec(shape=(265,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor'),\n",
       " ObservationSpec(shape=(245,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-3'),\n",
       " ObservationSpec(shape=(0,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor'),\n",
       " ObservationSpec(shape=(4,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size4')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.observation_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "for act in range(5):\n",
    "    actions.append(spec.action_spec.empty_action(1))\n",
    "    actions[act].add_discrete(np.int32([[act]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent, n_episodes=2, max_t=100, eps_start=1.0, eps_end=0.1, eps_decay=0.99):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "#     rewards =0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # every episode we reset the environment to start state\n",
    "        env.reset()\n",
    "#         print(\"~~~~~~~~~~~~~~~~~~~~\")\n",
    "        decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "        ray_sensor_1 = decision_steps.obs[0]\n",
    "        ray_sensor_2 = decision_steps.obs[1]\n",
    "        state = np.concatenate((ray_sensor_1, ray_sensor_2), axis=1)\n",
    "        \n",
    "        tracked_agent = -1\n",
    "        done = False\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            reward = 0\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0]\n",
    "            action = agent.act(state, eps)\n",
    "            env.set_actions(brain_name, actions[action])\n",
    "            env.step()\n",
    "            \n",
    "            decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "            if len(decision_steps.obs[0]) != 1:\n",
    "                env.reset()\n",
    "            else: \n",
    "                ray_sensor_1 = decision_steps.obs[0]\n",
    "                \n",
    "            if len(decision_steps.obs[1]) != 1:\n",
    "                env.reset()\n",
    "            else: \n",
    "                ray_sensor_2 = decision_steps.obs[1]\n",
    "                \n",
    "            if len(decision_steps.obs[3]) == 0:\n",
    "                env.reset()\n",
    "            else: \n",
    "                battery = decision_steps.obs[3][0][0]\n",
    "\n",
    "            next_state = np.concatenate((ray_sensor_1, ray_sensor_2), axis=1)\n",
    "            \n",
    "            if battery == 0: # if the battery use up, give a penalty\n",
    "                reward = -2\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "            if tracked_agent in decision_steps:# The agent requested a decision\n",
    "                reward += decision_steps[tracked_agent].reward  # get the reward\n",
    "                agent.step(state, action, reward, next_state, False)\n",
    "            if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                reward += terminal_steps[tracked_agent].reward# get the reward\n",
    "                agent.step(state, action, reward, next_state, True)\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "        \n",
    "        scores_window.append(score)       # save most recent score\n",
    "#         print(scores_window)\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores)))\n",
    "        print('\\rEpisode {}\\tScore: {:.2f}'.format(i_episode, score))\n",
    "        if i_episode % 50 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            print('saved temporary learned weight')\n",
    "#         if np.mean(scores_window)>=500.0:\n",
    "#             print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "#             torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "#             print('agent done training')\n",
    "#             break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.00\n",
      "Episode 1\tScore: 0.00\n",
      "Episode 2\tAverage Score: 0.00\n",
      "Episode 2\tScore: 0.00\n",
      "Episode 3\tAverage Score: 0.00\n",
      "Episode 3\tScore: 0.00\n",
      "Episode 4\tAverage Score: 0.25\n",
      "Episode 4\tScore: 1.00\n",
      "Episode 5\tAverage Score: 0.20\n",
      "Episode 5\tScore: 0.00\n",
      "Episode 6\tAverage Score: 0.17\n",
      "Episode 6\tScore: 0.00\n",
      "Episode 7\tAverage Score: 0.14\n",
      "Episode 7\tScore: 0.00\n",
      "Episode 8\tAverage Score: 0.12\n",
      "Episode 8\tScore: 0.00\n",
      "Episode 9\tAverage Score: 0.11\n",
      "Episode 9\tScore: 0.00\n",
      "Episode 10\tAverage Score: 0.00\n",
      "Episode 10\tScore: -1.00\n"
     ]
    }
   ],
   "source": [
    "# if os.path.isfile('./checkpoint.pth'):\n",
    "#     # load the weights from file\n",
    "#     agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "    \n",
    "scores = train_dqn(agent, n_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Watch a trained agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def watch_banana_agent(agent, env, n_episodes=4, n_steps=300):\n",
    "\n",
    "                                   \n",
    "    \n",
    "#     for episode in range(n_episodes):\n",
    "        \n",
    "#         env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "#         state = env_info.vector_observations[0]            # get the current state\n",
    "#         score = 0                                          # initialize the score\n",
    "        \n",
    "#         for step in range(n_steps):\n",
    "\n",
    "#             action = agent.act(state)                 # select an action\n",
    "#             env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#             next_state = env_info.vector_observations[0]   # get the next state\n",
    "#             reward = env_info.rewards[0]                   # get the reward\n",
    "#             done = env_info.local_done[0]                  # see if episode has finished\n",
    "#             score += reward                                # update the score\n",
    "#             state = next_state                             # roll over the state to next time step\n",
    "#             if done:                                       # exit loop if episode finished\n",
    "#                 break\n",
    "\n",
    "#         print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch_banana_agent(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
