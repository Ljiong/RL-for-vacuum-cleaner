{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "from dqn_agent_all import Agent\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_PATH = \"environment-MAC/en.app\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialise customised Banana Collecter environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_env(ENV_PATH):\n",
    "    # env = UnityEnvironment(file_name=ENV_PATH)\n",
    "    env = UE(base_port=5004,file_name=ENV_PATH, seed=1, side_channels=[])\n",
    "    env.step()\n",
    "    # in this project, we are only using one agent, so we will only work on the first `brain` in the environmet\n",
    "    # get the default brain\n",
    "    # brain_name = env.brain_names[0]\n",
    "    brain_name = list(env.behavior_specs.keys())[0]\n",
    "    # brain = env.brains[brain_name]\n",
    "    brain = env.behavior_specs[brain_name]\n",
    "    return env, brain, brain_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0618 00:50:38.574656000 8644032000 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n"
     ]
    }
   ],
   "source": [
    "env, brain, brain_name = initialise_env(ENV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BehaviorSpec(observation_specs=[ObservationSpec(shape=(265,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor'), ObservationSpec(shape=(245,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-3'), ObservationSpec(shape=(0,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor'), ObservationSpec(shape=(4,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size4')], action_spec=ActionSpec(continuous_size=0, discrete_branches=(5,)))\n"
     ]
    }
   ],
   "source": [
    "spec = env.behavior_specs['My Behavior?team=0']\n",
    "print(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = list(brain.action_spec)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = len(brain.observation_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObservationSpec(shape=(265,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor'),\n",
       " ObservationSpec(shape=(245,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-3'),\n",
       " ObservationSpec(shape=(0,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor'),\n",
       " ObservationSpec(shape=(4,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size4')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.observation_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "for act in range(5):\n",
    "    actions.append(spec.action_spec.empty_action(1))\n",
    "    actions[act].add_discrete(np.int32([[act]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent, n_episodes=2, max_t=100, eps_start=1.0, eps_end=0.1, eps_decay=0.99):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "#     rewards =0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # every episode we reset the environment to start state\n",
    "        env.reset()\n",
    "#         print(\"~~~~~~~~~~~~~~~~~~~~\")\n",
    "        decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "        ray_sensor_1 = decision_steps.obs[0]\n",
    "        ray_sensor_2 = decision_steps.obs[1]\n",
    "        state = np.concatenate((ray_sensor_1, ray_sensor_2), axis=1)\n",
    "        \n",
    "        tracked_agent = -1\n",
    "        done = False\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            reward = 0\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0]\n",
    "            action = agent.act(state, eps)\n",
    "            env.set_actions(brain_name, actions[action])\n",
    "            env.step()\n",
    "            \n",
    "            decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "            if len(decision_steps.obs[0]) != 1:\n",
    "                env.reset()\n",
    "            else: \n",
    "                ray_sensor_1 = decision_steps.obs[0]\n",
    "                \n",
    "            if len(decision_steps.obs[1]) != 1:\n",
    "                env.reset()\n",
    "            else: \n",
    "                ray_sensor_2 = decision_steps.obs[1]\n",
    "                \n",
    "            if len(decision_steps.obs[3]) == 0:\n",
    "                env.reset()\n",
    "            else: \n",
    "                battery = decision_steps.obs[3][0][0]\n",
    "\n",
    "            next_state = np.concatenate((ray_sensor_1, ray_sensor_2), axis=1)\n",
    "            \n",
    "            if battery == 0: # if the battery use up, give a penalty\n",
    "                reward = -2\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "            if tracked_agent in decision_steps:# The agent requested a decision\n",
    "                reward += decision_steps[tracked_agent].reward  # get the reward\n",
    "                agent.step(state, action, reward, next_state, False)\n",
    "            if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                reward += terminal_steps[tracked_agent].reward# get the reward\n",
    "                agent.step(state, action, reward, next_state, True)\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "        \n",
    "        scores_window.append(score)       # save most recent score\n",
    "#         print(scores_window)\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores)))\n",
    "        print('\\rEpisode {}\\tScore: {:.2f}'.format(i_episode, score))\n",
    "        if i_episode % 50 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            print('saved temporary learned weight')\n",
    "#         if np.mean(scores_window)>=500.0:\n",
    "#             print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "#             torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "#             print('agent done training')\n",
    "#             break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.00\n",
      "Episode 1\tScore: 0.00\n",
      "Episode 2\tAverage Score: 0.50\n",
      "Episode 2\tScore: 1.00\n",
      "Episode 3\tAverage Score: 0.33\n",
      "Episode 3\tScore: 0.00\n",
      "Episode 4\tAverage Score: 0.25\n",
      "Episode 4\tScore: 0.00\n",
      "Episode 5\tAverage Score: -1.00\n",
      "Episode 5\tScore: -6.00\n"
     ]
    }
   ],
   "source": [
    "# if os.path.isfile('./checkpoint.pth'):\n",
    "#     # load the weights from file\n",
    "#     agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "    \n",
    "scores = train_dqn(agent, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Watch a trained agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def watch_banana_agent(agent, env, n_episodes=4, n_steps=300):\n",
    "\n",
    "                                   \n",
    "    \n",
    "#     for episode in range(n_episodes):\n",
    "        \n",
    "#         env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "#         state = env_info.vector_observations[0]            # get the current state\n",
    "#         score = 0                                          # initialize the score\n",
    "        \n",
    "#         for step in range(n_steps):\n",
    "\n",
    "#             action = agent.act(state)                 # select an action\n",
    "#             env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#             next_state = env_info.vector_observations[0]   # get the next state\n",
    "#             reward = env_info.rewards[0]                   # get the reward\n",
    "#             done = env_info.local_done[0]                  # see if episode has finished\n",
    "#             score += reward                                # update the score\n",
    "#             state = next_state                             # roll over the state to next time step\n",
    "#             if done:                                       # exit loop if episode finished\n",
    "#                 break\n",
    "\n",
    "#         print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch_banana_agent(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
