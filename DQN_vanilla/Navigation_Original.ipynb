{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "from dqn_agent_Original import Agent\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_PATH = \"environment-MAC/en.app\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialise customised Banana Collecter environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_env(ENV_PATH):\n",
    "    # env = UnityEnvironment(file_name=ENV_PATH)\n",
    "    env = UE(base_port=5004,file_name=ENV_PATH, seed=1, side_channels=[])\n",
    "    env.step()\n",
    "    # in this project, we are only using one agent, so we will only work on the first `brain` in the environmet\n",
    "    # get the default brain\n",
    "    # brain_name = env.brain_names[0]\n",
    "    brain_name = list(env.behavior_specs.keys())[0]\n",
    "    # brain = env.brains[brain_name]\n",
    "    brain = env.behavior_specs[brain_name]\n",
    "    return env, brain, brain_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n"
     ]
    }
   ],
   "source": [
    "env, brain, brain_name = initialise_env(ENV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = env.behavior_specs['My Behavior?team=0']\n",
    "# print(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = list(brain.action_spec)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = len(brain.observation_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObservationSpec(shape=(265,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor'),\n",
       " ObservationSpec(shape=(245,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-3'),\n",
       " ObservationSpec(shape=(0,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor'),\n",
       " ObservationSpec(shape=(4,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size4')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.observation_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "for act in range(5):\n",
    "    actions.append(spec.action_spec.empty_action(1))\n",
    "    actions[act].add_discrete(np.int32([[act]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent, n_episodes=2, max_t=100, eps_start=1.0, eps_end=0.1, eps_decay=0.99):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "#     rewards =0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # every episode we reset the environment to start state\n",
    "        env.reset()\n",
    "#         print(\"~~~~~~~~~~~~~~~~~~~~\")\n",
    "        decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "        ray_sensor_1 = decision_steps.obs[0]\n",
    "        ray_sensor_2 = decision_steps.obs[1]\n",
    "        state = np.concatenate((ray_sensor_1, ray_sensor_2), axis=1)\n",
    "        \n",
    "        tracked_agent = -1\n",
    "        done = False\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            reward = 0\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0]\n",
    "            action = agent.act(state, eps)\n",
    "            env.set_actions(brain_name, actions[action])\n",
    "            env.step()\n",
    "            \n",
    "            decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "            if len(decision_steps.obs[0]) != 1:\n",
    "                env.reset()\n",
    "            else: \n",
    "                ray_sensor_1 = decision_steps.obs[0]\n",
    "                \n",
    "            if len(decision_steps.obs[1]) != 1:\n",
    "                env.reset()\n",
    "            else: \n",
    "                ray_sensor_2 = decision_steps.obs[1]\n",
    "                \n",
    "            if len(decision_steps.obs[3]) == 0:\n",
    "                env.reset()\n",
    "            else: \n",
    "                battery = decision_steps.obs[3][0][0]\n",
    "\n",
    "            next_state = np.concatenate((ray_sensor_1, ray_sensor_2), axis=1)\n",
    "            \n",
    "            if battery == 0: # if the battery use up, give a penalty\n",
    "                reward = -2\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "            if tracked_agent in decision_steps:# The agent requested a decision\n",
    "                reward += decision_steps[tracked_agent].reward  # get the reward\n",
    "                agent.step(state, action, reward, next_state, False)\n",
    "            if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                reward += terminal_steps[tracked_agent].reward# get the reward\n",
    "                agent.step(state, action, reward, next_state, True)\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "        \n",
    "        scores_window.append(score)       # save most recent score\n",
    "#         print(scores_window)\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores)))\n",
    "        print('\\rEpisode {}\\tScore: {:.2f}'.format(i_episode, score))\n",
    "        if i_episode % 50 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            print('saved temporary learned weight')\n",
    "#         if np.mean(scores_window)>=500.0:\n",
    "#             print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "#             torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "#             print('agent done training')\n",
    "#             break\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.00\n",
      "Episode 1\tScore: 0.00\n",
      "Episode 2\tAverage Score: 0.00\n",
      "Episode 2\tScore: 0.00\n",
      "Episode 3\tAverage Score: 0.00\n",
      "Episode 3\tScore: 0.00\n",
      "Episode 4\tAverage Score: 0.25\n",
      "Episode 4\tScore: 1.00\n",
      "Episode 5\tAverage Score: 0.20\n",
      "Episode 5\tScore: 0.00\n"
     ]
    }
   ],
   "source": [
    "# if os.path.isfile('./checkpoint.pth'):\n",
    "#     # load the weights from file\n",
    "#     agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "    \n",
    "scores = train_dqn(agent, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Watch a trained agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def watch_banana_agent(agent, env, n_episodes=4, n_steps=300):\n",
    "\n",
    "                                   \n",
    "    \n",
    "#     for episode in range(n_episodes):\n",
    "        \n",
    "#         env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "#         state = env_info.vector_observations[0]            # get the current state\n",
    "#         score = 0                                          # initialize the score\n",
    "        \n",
    "#         for step in range(n_steps):\n",
    "\n",
    "#             action = agent.act(state)                 # select an action\n",
    "#             env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#             next_state = env_info.vector_observations[0]   # get the next state\n",
    "#             reward = env_info.rewards[0]                   # get the reward\n",
    "#             done = env_info.local_done[0]                  # see if episode has finished\n",
    "#             score += reward                                # update the score\n",
    "#             state = next_state                             # roll over the state to next time step\n",
    "#             if done:                                       # exit loop if episode finished\n",
    "#                 break\n",
    "\n",
    "#         print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch_banana_agent(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
