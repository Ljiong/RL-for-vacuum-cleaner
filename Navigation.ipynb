{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "from dqn_agent import Agent\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_PATH = \"environment-MAC/en.app\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialise customised Banana Collecter environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_env(ENV_PATH):\n",
    "    # env = UnityEnvironment(file_name=ENV_PATH)\n",
    "    env = UE(base_port=5004,file_name=ENV_PATH, seed=1, side_channels=[])\n",
    "    env.step()\n",
    "    # in this project, we are only using one agent, so we will only work on the first `brain` in the environmet\n",
    "    # get the default brain\n",
    "    # brain_name = env.brain_names[0]\n",
    "    brain_name = list(env.behavior_specs.keys())[0]\n",
    "    # brain = env.brains[brain_name]\n",
    "    brain = env.behavior_specs[brain_name]\n",
    "    return env, brain, brain_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = UE(file_name=ENV_PATH, seed=1, side_channels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = env.behavior_specs.keys()\n",
    "# print(list(test)[0])\n",
    "# for t in test:\n",
    "#     print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brain_name = list(env.behavior_specs.keys())[0]\n",
    "# print(brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brain = env.behavior_specs[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0614 16:07:11.126702000 8640914944 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n"
     ]
    }
   ],
   "source": [
    "env, brain, brain_name = initialise_env(ENV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先让他走几步直到需要交互\n",
    "# env.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BehaviorSpec(observation_specs=[ObservationSpec(shape=(84, 84, 3), dimension_property=(<DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.NONE: 1>), observation_type=<ObservationType.DEFAULT: 0>, name='CameraSensor'), ObservationSpec(shape=(0,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor'), ObservationSpec(shape=(4,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size4')], action_spec=ActionSpec(continuous_size=0, discrete_branches=(5,)))\n"
     ]
    }
   ],
   "source": [
    "spec = env.behavior_specs['My Behavior?team=0']\n",
    "print(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "#env_info = env.reset(train_mode=True)[brain_name]\n",
    "env_info = env.reset()\n",
    "#action_size = brain.vector_action_space_size\n",
    "#state_size = len(env_info.vector_observations[0])\n",
    "#agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(env_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = list(brain.action_spec)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change\n",
    "# decision_steps.obs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ObservationSpec(shape=(84, 84, 3), dimension_property=(<DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.TRANSLATIONAL_EQUIVARIANCE: 2>, <DimensionProperty.NONE: 1>), observation_type=<ObservationType.DEFAULT: 0>, name='CameraSensor'), ObservationSpec(shape=(0,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor'), ObservationSpec(shape=(4,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size4')]\n"
     ]
    }
   ],
   "source": [
    "print(brain.observation_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = len(brain.observation_specs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_size = len(env_info.vector_observations[0])\n",
    "# state size 要改\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms as transforms\n",
    "actions = []\n",
    "for act in range(5):\n",
    "    actions.append(spec.action_spec.empty_action(1))\n",
    "    actions[act].add_discrete(np.int32([[act]]))\n",
    "stop = actions[0]\n",
    "forward = actions[1]\n",
    "backward = actions[2]\n",
    "turn_right = actions[3]\n",
    "turn_left = actions[4]\n",
    "def train_dqn(agent, n_episodes=2, max_t=10, eps_start=1.0, eps_end=0.1, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    rewards =0\n",
    "    reward =0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "        state = np.moveaxis(decision_steps.obs[0], -1, 0)\n",
    "        tracked_agent = -1\n",
    "        done = False\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0]\n",
    "            if not state.shape == (3, 1, 84, 84):\n",
    "                continue\n",
    "                \n",
    "            action = agent.act(state, eps)\n",
    "            env.set_actions(brain_name, actions[action])\n",
    "            env.step()\n",
    "            decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "            \n",
    "            next_state = np.moveaxis(decision_steps.obs[0], -1, 0)   # get the next state\n",
    "            \n",
    "            if tracked_agent in decision_steps:# The agent requested a decision\n",
    "                reward += decision_steps[tracked_agent].reward  # get the reward\n",
    "                agent.step(state, action, reward, next_state, False)\n",
    "            if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                rewards += terminal_steps[tracked_agent].reward# get the reward\n",
    "                agent.step(state, action, reward, next_state, True)\n",
    "                break\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "        print(score)\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        print(scores_window)\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "#         if i_episode % 100 == 0:\n",
    "#             print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "#             torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "#             print('saved temporary learned weight')\n",
    "#         if np.mean(scores_window)>=13.0:\n",
    "#             print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "#             torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "#             print('agent done training')\n",
    "#             break\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "deque([100.0], maxlen=100)\n",
      "Episode 1\tAverage Score: 100.0095.0\n",
      "deque([100.0, 95.0], maxlen=100)\n",
      "Episode 2\tAverage Score: 97.5084.0\n",
      "deque([100.0, 95.0, 84.0], maxlen=100)\n",
      "Episode 3\tAverage Score: 93.0073.0\n",
      "deque([100.0, 95.0, 84.0, 73.0], maxlen=100)\n",
      "Episode 4\tAverage Score: 88.0070.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0], maxlen=100)\n",
      "Episode 5\tAverage Score: 84.4070.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0], maxlen=100)\n",
      "Episode 6\tAverage Score: 82.0066.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0], maxlen=100)\n",
      "Episode 7\tAverage Score: 79.7150.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0], maxlen=100)\n",
      "Episode 8\tAverage Score: 76.00110.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0], maxlen=100)\n",
      "Episode 9\tAverage Score: 79.78150.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0], maxlen=100)\n",
      "Episode 10\tAverage Score: 86.80150.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0], maxlen=100)\n",
      "Episode 11\tAverage Score: 92.55150.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0], maxlen=100)\n",
      "Episode 12\tAverage Score: 97.33139.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0], maxlen=100)\n",
      "Episode 13\tAverage Score: 100.54127.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0], maxlen=100)\n",
      "Episode 14\tAverage Score: 102.43110.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0], maxlen=100)\n",
      "Episode 15\tAverage Score: 102.93110.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0], maxlen=100)\n",
      "Episode 16\tAverage Score: 103.38110.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0], maxlen=100)\n",
      "Episode 17\tAverage Score: 103.76110.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0], maxlen=100)\n",
      "Episode 18\tAverage Score: 104.11110.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0], maxlen=100)\n",
      "Episode 19\tAverage Score: 104.42110.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0], maxlen=100)\n",
      "Episode 20\tAverage Score: 104.70110.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0], maxlen=100)\n",
      "Episode 21\tAverage Score: 104.95104.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0], maxlen=100)\n",
      "Episode 22\tAverage Score: 104.9190.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0], maxlen=100)\n",
      "Episode 23\tAverage Score: 104.2685.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0], maxlen=100)\n",
      "Episode 24\tAverage Score: 103.4678.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0], maxlen=100)\n",
      "Episode 25\tAverage Score: 102.4480.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0], maxlen=100)\n",
      "Episode 26\tAverage Score: 101.58220.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0], maxlen=100)\n",
      "Episode 27\tAverage Score: 105.96260.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0], maxlen=100)\n",
      "Episode 28\tAverage Score: 111.46260.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0], maxlen=100)\n",
      "Episode 29\tAverage Score: 116.59260.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0], maxlen=100)\n",
      "Episode 30\tAverage Score: 121.37350.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0], maxlen=100)\n",
      "Episode 31\tAverage Score: 128.74360.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0], maxlen=100)\n",
      "Episode 32\tAverage Score: 135.97360.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0, 360.0], maxlen=100)\n",
      "Episode 33\tAverage Score: 142.76360.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0, 360.0, 360.0], maxlen=100)\n",
      "Episode 34\tAverage Score: 149.15352.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0, 360.0, 360.0, 352.0], maxlen=100)\n",
      "Episode 35\tAverage Score: 154.94341.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0, 360.0, 360.0, 352.0, 341.0], maxlen=100)\n",
      "Episode 36\tAverage Score: 160.11326.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0, 360.0, 360.0, 352.0, 341.0, 326.0], maxlen=100)\n",
      "Episode 37\tAverage Score: 164.59310.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0, 360.0, 360.0, 352.0, 341.0, 326.0, 310.0], maxlen=100)\n",
      "Episode 38\tAverage Score: 168.42294.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0, 360.0, 360.0, 352.0, 341.0, 326.0, 310.0, 294.0], maxlen=100)\n",
      "Episode 39\tAverage Score: 171.64290.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0, 360.0, 360.0, 352.0, 341.0, 326.0, 310.0, 294.0, 290.0], maxlen=100)\n",
      "Episode 40\tAverage Score: 174.60290.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0, 360.0, 360.0, 352.0, 341.0, 326.0, 310.0, 294.0, 290.0, 290.0], maxlen=100)\n",
      "Episode 41\tAverage Score: 177.41290.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0, 360.0, 360.0, 352.0, 341.0, 326.0, 310.0, 294.0, 290.0, 290.0, 290.0], maxlen=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 42\tAverage Score: 180.10290.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0, 360.0, 360.0, 352.0, 341.0, 326.0, 310.0, 294.0, 290.0, 290.0, 290.0, 290.0], maxlen=100)\n",
      "Episode 43\tAverage Score: 182.65290.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0, 360.0, 360.0, 352.0, 341.0, 326.0, 310.0, 294.0, 290.0, 290.0, 290.0, 290.0, 290.0], maxlen=100)\n",
      "Episode 44\tAverage Score: 185.09290.0\n",
      "deque([100.0, 95.0, 84.0, 73.0, 70.0, 70.0, 66.0, 50.0, 110.0, 150.0, 150.0, 150.0, 139.0, 127.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 110.0, 104.0, 90.0, 85.0, 78.0, 80.0, 220.0, 260.0, 260.0, 260.0, 350.0, 360.0, 360.0, 360.0, 352.0, 341.0, 326.0, 310.0, 294.0, 290.0, 290.0, 290.0, 290.0, 290.0, 290.0], maxlen=100)\n",
      "Episode 45\tAverage Score: 187.42"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('./checkpoint.pth'):\n",
    "    # load the weights from file\n",
    "    agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "    \n",
    "scores = train_dqn(agent, n_episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Watch a trained agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def watch_banana_agent(agent, env, n_episodes=4, n_steps=300):\n",
    "\n",
    "                                   \n",
    "    \n",
    "#     for episode in range(n_episodes):\n",
    "        \n",
    "#         env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "#         state = env_info.vector_observations[0]            # get the current state\n",
    "#         score = 0                                          # initialize the score\n",
    "        \n",
    "#         for step in range(n_steps):\n",
    "\n",
    "#             action = agent.act(state)                 # select an action\n",
    "#             env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#             next_state = env_info.vector_observations[0]   # get the next state\n",
    "#             reward = env_info.rewards[0]                   # get the reward\n",
    "#             done = env_info.local_done[0]                  # see if episode has finished\n",
    "#             score += reward                                # update the score\n",
    "#             state = next_state                             # roll over the state to next time step\n",
    "#             if done:                                       # exit loop if episode finished\n",
    "#                 break\n",
    "\n",
    "#         print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reset() got an unexpected keyword argument 'train_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p2/fj_ptp_x055gqt839fj3ws6m0000gn/T/ipykernel_67920/1660049170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwatch_banana_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/p2/fj_ptp_x055gqt839fj3ws6m0000gn/T/ipykernel_67920/2633861598.py\u001b[0m in \u001b[0;36mwatch_banana_agent\u001b[0;34m(agent, env, n_episodes, n_steps)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# reset the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m            \u001b[0;31m# get the current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m                                          \u001b[0;31m# initialize the score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: reset() got an unexpected keyword argument 'train_mode'"
     ]
    }
   ],
   "source": [
    "# watch_banana_agent(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
