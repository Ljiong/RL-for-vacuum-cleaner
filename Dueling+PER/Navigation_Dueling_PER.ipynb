{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "from dqn_agent_Dueling_PER import Agent\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_PATH = \"environment-MAC/en.app\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialise customised Banana Collecter environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_env(ENV_PATH):\n",
    "    # env = UnityEnvironment(file_name=ENV_PATH)\n",
    "    env = UE(base_port=5004,file_name=ENV_PATH, seed=1, side_channels=[])\n",
    "    env.step()\n",
    "    # in this project, we are only using one agent, so we will only work on the first `brain` in the environmet\n",
    "    # get the default brain\n",
    "    # brain_name = env.brain_names[0]\n",
    "    brain_name = list(env.behavior_specs.keys())[0]\n",
    "    # brain = env.brains[brain_name]\n",
    "    brain = env.behavior_specs[brain_name]\n",
    "    return env, brain, brain_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0618 00:43:46.386245000 8613520896 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n"
     ]
    }
   ],
   "source": [
    "env, brain, brain_name = initialise_env(ENV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = env.behavior_specs['My Behavior?team=0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = list(brain.action_spec)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = len(brain.observation_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ObservationSpec(shape=(265,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor'),\n",
       " ObservationSpec(shape=(245,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='RayPerceptionSensor-3'),\n",
       " ObservationSpec(shape=(0,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor'),\n",
       " ObservationSpec(shape=(4,), dimension_property=(<DimensionProperty.NONE: 1>,), observation_type=<ObservationType.DEFAULT: 0>, name='VectorSensor_size4')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.observation_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "for act in range(5):\n",
    "    actions.append(spec.action_spec.empty_action(1))\n",
    "    actions[act].add_discrete(np.int32([[act]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent, n_episodes=2, max_t=100, eps_start=1.0, eps_end=0.1, eps_decay=0.99):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "#     rewards =0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # every episode we reset the environment to start state\n",
    "        env.reset()\n",
    "#         print(\"~~~~~~~~~~~~~~~~~~~~\")\n",
    "        decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "        ray_sensor_1 = decision_steps.obs[0]\n",
    "        ray_sensor_2 = decision_steps.obs[1]\n",
    "        state = np.concatenate((ray_sensor_1, ray_sensor_2), axis=1)\n",
    "        \n",
    "        tracked_agent = -1\n",
    "        done = False\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            reward = 0\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0]\n",
    "            action = agent.act(state, eps)\n",
    "            env.set_actions(brain_name, actions[action])\n",
    "            env.step()\n",
    "            \n",
    "            decision_steps, terminal_steps = env.get_steps(brain_name)\n",
    "            if len(decision_steps.obs[0]) != 265:\n",
    "                env.reset()\n",
    "            else: \n",
    "                ray_sensor_1 = decision_steps.obs[0]\n",
    "                \n",
    "            if len(decision_steps.obs[1]) != 245:\n",
    "                env.reset()\n",
    "            else: \n",
    "                ray_sensor_2 = decision_steps.obs[1]\n",
    "                \n",
    "            if len(decision_steps.obs[3]) == 0:\n",
    "                env.reset()\n",
    "            else: \n",
    "                battery = decision_steps.obs[3][0][0]\n",
    "\n",
    "            next_state = np.concatenate((ray_sensor_1, ray_sensor_2), axis=1)\n",
    "            \n",
    "            if battery == 0: # if the battery use up, give a penalty\n",
    "                reward = -2\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "            if tracked_agent in decision_steps:# The agent requested a decision\n",
    "                reward += decision_steps[tracked_agent].reward  # get the reward\n",
    "                agent.step(state, action, reward, next_state, False)\n",
    "            if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                reward += terminal_steps[tracked_agent].reward# get the reward\n",
    "                agent.step(state, action, reward, next_state, True)\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "        \n",
    "        scores_window.append(score)       # save most recent score\n",
    "#         print(scores_window)\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores)))\n",
    "        print('\\rEpisode {}\\tScore: {:.2f}'.format(i_episode, score))\n",
    "        if i_episode % 50 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            print('saved temporary learned weight')\n",
    "#         if np.mean(scores_window)>=500.0:\n",
    "#             print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "#             torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "#             print('agent done training')\n",
    "#             break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.00\n",
      "Episode 1\tScore: 0.00\n",
      "Episode 2\tAverage Score: 0.50\n",
      "Episode 2\tScore: 1.00\n",
      "Episode 3\tAverage Score: 0.33\n",
      "Episode 3\tScore: 0.00\n",
      "Episode 4\tAverage Score: 0.25\n",
      "Episode 4\tScore: 0.00\n",
      "Episode 5\tAverage Score: -1.00\n",
      "Episode 5\tScore: -6.00\n",
      "Episode 6\tAverage Score: -0.67\n",
      "Episode 6\tScore: 1.00\n",
      "Episode 7\tAverage Score: -0.57\n",
      "Episode 7\tScore: 0.00\n",
      "Episode 8\tAverage Score: -0.50\n",
      "Episode 8\tScore: 0.00\n",
      "Episode 9\tAverage Score: -0.44\n",
      "Episode 9\tScore: 0.00\n",
      "Episode 10\tAverage Score: -0.40\n",
      "Episode 10\tScore: 0.00\n"
     ]
    }
   ],
   "source": [
    "# if os.path.isfile('./checkpoint.pth'):\n",
    "#     # load the weights from file\n",
    "#     agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "    \n",
    "scores = train_dqn(agent, n_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Watch a trained agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def watch_banana_agent(agent, env, n_episodes=4, n_steps=300):\n",
    "\n",
    "                                   \n",
    "    \n",
    "#     for episode in range(n_episodes):\n",
    "        \n",
    "#         env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "#         state = env_info.vector_observations[0]            # get the current state\n",
    "#         score = 0                                          # initialize the score\n",
    "        \n",
    "#         for step in range(n_steps):\n",
    "\n",
    "#             action = agent.act(state)                 # select an action\n",
    "#             env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "#             next_state = env_info.vector_observations[0]   # get the next state\n",
    "#             reward = env_info.rewards[0]                   # get the reward\n",
    "#             done = env_info.local_done[0]                  # see if episode has finished\n",
    "#             score += reward                                # update the score\n",
    "#             state = next_state                             # roll over the state to next time step\n",
    "#             if done:                                       # exit loop if episode finished\n",
    "#                 break\n",
    "\n",
    "#         print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch_banana_agent(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
